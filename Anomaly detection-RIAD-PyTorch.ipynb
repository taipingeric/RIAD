{"cells":[{"cell_type":"markdown","metadata":{"id":"hFlLC2An1_CO"},"source":["#### Downlaod MvTec (toothbrush) dataset"]},{"cell_type":"markdown","metadata":{"id":"koEdOEHkN9C7"},"source":["#### Clone repo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EVQ8qkbaGgGP"},"outputs":[],"source":["!git clone https://github.com/taikiinoue45/RIAD.git\n","import sys\n","sys.path.append(\"RIAD/riad\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wt-xa4sT47js"},"outputs":[],"source":["import cv2\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from glob import glob\n","from tqdm.auto import tqdm\n","import math\n","\n","import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n"]},{"cell_type":"markdown","metadata":{"id":"SgwjyphT5ZXf"},"source":["#### EDA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bG6By2F75bIV"},"outputs":[],"source":["normal_paths = glob(\"toothbrush/train/good/*.png\")\n","\n","img = cv2.imread(normal_paths[0])\n","img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","plt.imshow(img)\n","plt.show()\n","\n","len(normal_paths)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ltdi21RZBbQZ"},"outputs":[],"source":["defect_paths = sorted(glob(\"toothbrush/test/defective/*.png\"))\n","mask_paths = sorted(glob(\"toothbrush/ground_truth/defective/*_mask.png\"))\n","\n","for i, (p1, p2) in enumerate(zip(defect_paths, mask_paths)):\n","    img = cv2.imread(p1)\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    mask = cv2.imread(p2)\n","    plt.subplot(1, 2, 1)\n","    plt.imshow(img)\n","    plt.subplot(1, 2, 2)\n","    plt.imshow(mask)\n","    plt.show()\n","    if i > 4:\n","        break"]},{"cell_type":"markdown","metadata":{"id":"v6omvpO65NF8"},"source":["#### Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xUzxX5G95DiV"},"outputs":[],"source":["from models import UNet"]},{"cell_type":"markdown","metadata":{"id":"ztNK3Q3T59nD"},"source":["#### Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cFEv281G5OK6"},"outputs":[],"source":["class MVTecDataset(torch.utils.data.Dataset):\n","    def __init__(self, img_paths, transform, img_size=256):\n","        self.img_paths = img_paths\n","        self.transform = transform\n","        self.img_size = 256\n","\n","    def __len__(self):\n","        return len(self.img_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.img_paths[idx]\n","        img = cv2.imread(img_path)\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        img = cv2.resize(img, (self.img_size, self.img_size))\n","        img = self.transform(img)\n","        return img\n","\n","BS = 8\n","# Preprocess Transform\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(\n","        mean=[0.5, 0.5, 0.5], \n","        std=[0.5, 0.5, 0.5])\n","])\n","\n","normal_paths = glob(\"toothbrush/train/good/*.png\")\n","train_ds = MVTecDataset(normal_paths, transform)\n","train_loader = torch.utils.data.DataLoader(train_ds, BS, shuffle=True)\n","\n","val_normal_paths = glob(\"toothbrush/test/good/*.png\")\n","val_defect_paths = glob(\"toothbrush/test/defective/*.png\")\n","\n","len(train_ds), len(val_normal_paths), len(val_defect_paths)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0GY6ZqLy8Ddf"},"outputs":[],"source":["img = train_ds[0]\n","img = (img*0.5 + 0.5).permute(1, 2, 0)\n","\n","plt.imshow(img)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"BXdHvMupDrYu"},"source":["#### Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pJvnK-YXDsw5"},"outputs":[],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using: {device}\")\n","cutout_sizes = [2, 4, 8, 16]\n","num_disjoint_masks = 3\n","\n","model = UNet().to(device)\n","\n","optimizer = torch.optim.Adam(model.parameters())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o4AZWr8WEMDO"},"outputs":[],"source":["def reconstruct(model, mb_img, cutout_size, num_disjoint_masks):\n","    _, _, h, w = mb_img.shape\n","    num_disjoint_masks = num_disjoint_masks\n","    disjoint_masks = create_disjoint_masks((h, w), cutout_size, num_disjoint_masks)\n","\n","    mb_reconst = 0\n","    for mask in disjoint_masks:\n","        mb_cutout = mb_img * mask\n","        mb_inpaint = model(mb_cutout)\n","        mb_reconst += mb_inpaint * (1 - mask)\n","\n","    return mb_reconst\n","\n","def create_disjoint_masks(\n","    img_size,\n","    cutout_size = 8,\n","    num_disjoint_masks = 3,\n","):\n","    img_h, img_w = img_size\n","    grid_h = math.ceil(img_h / cutout_size)\n","    grid_w = math.ceil(img_w / cutout_size)\n","    num_grids = grid_h * grid_w\n","    disjoint_masks = []\n","    for grid_ids in np.array_split(np.random.permutation(num_grids), num_disjoint_masks):\n","        flatten_mask = np.ones(num_grids)\n","        flatten_mask[grid_ids] = 0\n","        mask = flatten_mask.reshape((grid_h, grid_w))\n","        mask = mask.repeat(cutout_size, axis=0).repeat(cutout_size, axis=1)\n","        mask = torch.tensor(mask, requires_grad=False, dtype=torch.float)\n","        mask = mask.to(device)\n","        disjoint_masks.append(mask)\n","\n","    return disjoint_masks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZL9a9Sf_GMiR"},"outputs":[],"source":["from criterions import MSGMSLoss, SSIMLoss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gnk3VO9P8Ohv"},"outputs":[],"source":["def train_epoch(dataloader, model, optimizer):\n","    size = len(dataloader.dataset) # number of samples\n","    num_batches = len(dataloader) # batches per epoch\n","\n","    model.train() # to training mode.\n","    epoch_loss = 0\n","    for batch_i, img in enumerate(tqdm(dataloader)):\n","        optimizer.zero_grad()\n","\n","        img = img.to(device)\n","        cutout_size = np.random.choice(cutout_sizes)\n","        img_reconstruct = reconstruct(model, img, cutout_size, num_disjoint_masks)\n","\n","        loss_mse = nn.MSELoss()(img, img_reconstruct)\n","        loss_msgms = MSGMSLoss()(img, img_reconstruct)\n","        loss_ssim = SSIMLoss()(img, img_reconstruct)\n","        loss_total = loss_mse + loss_msgms + loss_ssim\n","\n","        loss_total.backward()\n","        optimizer.step()\n","\n","        epoch_loss += loss_total.item()\n","    return epoch_loss/num_batches"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-zLZZMohIZXp","tags":[]},"outputs":[],"source":["model = model.to(device)\n","EPOCHS = 300\n","logs = {\n","    'train_loss': []\n","}\n","best_loss = np.inf\n","\n","for epoch in tqdm(range(EPOCHS)):\n","    train_loss = train_epoch(train_loader, model, optimizer)\n","    \n","    print(f'EPOCH: {epoch:04d} train_loss: {train_loss:.4f}')\n","\n","    logs['train_loss'].append(train_loss)\n","    \n","    # On epoch end\n","    torch.save(model.state_dict(), \"last.pth\")\n","    # check improvement\n","    if train_loss < best_loss:\n","        best_loss = train_loss\n","        torch.save(model.state_dict(), \"best.pth\")"]},{"cell_type":"markdown","metadata":{"id":"wDRlC0q7N9C-"},"source":["#### Download pre-trained model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"16i5UQ6YN9C-"},"outputs":[],"source":["!gdown --fuzzy 164Kcrmlk3-wo5UX95YMcCT8nDv_AcHjk"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ziUJwiUUIx6I"},"outputs":[],"source":["# model.load_state_dict(torch.load(\"best.pth\"))\n","model.load_state_dict(torch.load(\"mvtec-tooth-riad.pth\"))\n","_ = model.eval()"]},{"cell_type":"markdown","metadata":{"id":"lQsOyt8RN9C-"},"source":["#### Test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UOftY5crN9C-"},"outputs":[],"source":["# Dataset for inference\n","class MVTecDefectDataset(torch.utils.data.Dataset):\n","    def __init__(self, img_paths, mask_paths, transform, img_size=256):\n","        self.img_paths = img_paths\n","        self.mask_paths = mask_paths\n","        self.transform = transform\n","        self.img_size = 256\n","\n","    def __len__(self):\n","        return len(self.img_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.img_paths[idx]\n","        img = cv2.imread(img_path)\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        img = cv2.resize(img, (self.img_size, self.img_size))\n","        img = self.transform(img)\n","        \n","        if self.mask_paths:\n","            mask_path = self.mask_paths[idx]\n","            mask = cv2.imread(mask_path)\n","            mask = cv2.resize(mask, (self.img_size, self.img_size))\n","            mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n","            mask = mask / 255.\n","            mask = torch.tensor(mask, dtype=torch.float)\n","        else:\n","            c, h, w = img.shape\n","            mask = torch.zeros((h, w), dtype=torch.float) \n","        return img, mask\n","\n","# defect image with mask    \n","defect_paths = sorted(glob(\"toothbrush/test/defective/*.png\"))\n","mask_paths = sorted(glob(\"toothbrush/ground_truth/defective/*_mask.png\"))\n","val_defect_ds = MVTecDefectDataset(defect_paths, mask_paths, transform)\n","\n","# normal image with black mask\n","val_normal_paths = glob(\"toothbrush/test/good/*.png\")\n","val_normal_ds = MVTecDefectDataset(val_normal_paths, None, transform)\n","\n","# combine defect and normal dataset\n","val_ds = torch.utils.data.ConcatDataset([val_defect_ds, val_normal_ds])\n","val_loader = torch.utils.data.DataLoader(val_ds, BS, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6frVnPDJN9C-"},"outputs":[],"source":["img, mask = val_ds[0]\n","img.shape, mask.shape\n","\n","img = (img*0.5 + 0.5).permute(1, 2, 0)\n","plt.imshow(img)\n","plt.show()\n","plt.imshow(mask)\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HDaKCTbUN9C-"},"outputs":[],"source":["import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","from numpy import ndarray as NDArray\n","from sklearn.metrics import roc_auc_score, roc_curve\n","\n","def mean_smoothing(amaps, kernel_size=21):\n","    mean_kernel = torch.ones(1, 1, kernel_size, kernel_size) / kernel_size ** 2\n","    mean_kernel = mean_kernel.to(amaps.device)\n","    return F.conv2d(amaps, mean_kernel, padding=kernel_size // 2, groups=1)\n","\n","def compute_auroc(epoch, ep_reconst, ep_gt):\n","    num_data = len(ep_reconst)\n","    y_score = ep_reconst.reshape(num_data, -1).max(axis=1)  # y_score.shape -> (num_data,)\n","    y_true = ep_gt.reshape(num_data, -1).max(axis=1)  # y_true.shape -> (num_data,)\n","    score = roc_auc_score(y_true, y_score)\n","    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n","    plt.plot(fpr, tpr, marker=\"o\", label=f\"AUROC Score: {round(score, 3)}\")\n","    plt.xlabel(\"FPR: FP / (TN + FP)\", fontsize=14)\n","    plt.ylabel(\"TPR: TP / (TP + FN)\", fontsize=14)\n","    plt.legend(fontsize=14)\n","    plt.tight_layout()\n","    plt.show()\n","\n","    return score\n","\n","\n","artifacts = {\n","    \"img\": [],\n","    \"reconst\": [],\n","    \"gt\": [],\n","    \"amap\": [],\n","}\n","for mb_img, mb_gt in tqdm(val_loader):\n","    mb_amap = 0\n","    with torch.no_grad():\n","        for cutout_size in cutout_sizes:\n","            mb_img = mb_img.to(device)\n","            mb_reconst = reconstruct(model, mb_img, cutout_size, num_disjoint_masks)\n","            mb_amap += MSGMSLoss()(mb_img, mb_reconst, as_loss=False)\n","\n","    mb_amap = mean_smoothing(mb_amap)\n","    artifacts[\"amap\"].extend(mb_amap.squeeze(1).detach().cpu().numpy())\n","    mb_img = mb_img*0.5 + 0.5\n","    artifacts[\"img\"].extend(mb_img.permute(0, 2, 3, 1).detach().cpu().numpy())\n","    mb_reconst = mb_reconst*0.5 + 0.5\n","    artifacts[\"reconst\"].extend(mb_reconst.permute(0, 2, 3, 1).detach().cpu().numpy())\n","    artifacts[\"gt\"].extend(mb_gt.detach().cpu().numpy())\n","\n","ep_amap = np.array(artifacts[\"amap\"])\n","ep_amap = (ep_amap - ep_amap.min()) / (ep_amap.max() - ep_amap.min())\n","artifacts[\"amap\"] = list(ep_amap)\n","\n","auroc = compute_auroc(epoch, np.array(artifacts[\"amap\"]), np.array(artifacts[\"gt\"]))"]},{"cell_type":"markdown","metadata":{"id":"e0HTWHqrPLbY"},"source":["#### Visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ShTgCR6jN9C_"},"outputs":[],"source":["for i, (amap, img, recons, gt) in enumerate(zip(artifacts[\"amap\"], artifacts[\"img\"], artifacts[\"reconst\"], artifacts[\"gt\"])):\n","    plt.figure(figsize=(20, 5))\n","    plt.subplot(1, 4, 1)\n","    plt.title(\"AMAP\")\n","    plt.imshow(amap, cmap=\"jet\", vmin=0, vmax=1)\n","    plt.subplot(1, 4, 2)\n","    plt.title(\"Img\")\n","    plt.imshow(img)\n","    plt.subplot(1, 4, 3)\n","    plt.title(\"Reconstruction\")\n","    plt.imshow(recons)\n","    plt.subplot(1, 4, 4)\n","    plt.title(\"GT\")\n","    plt.imshow(gt)\n","    plt.show()\n","    \n","    plt.figure(figsize=(20, 4))\n","    plt.hist(amap.ravel())\n","    plt.title(f\"AMP value min: {amap.min():.4f}, max: {amap.max():.4f}, mean: {amap.mean():.4f}\")\n","    plt.xlim(xmin=0., xmax = 1.)\n","    plt.axvline(amap.max(), color='red', linestyle='dashed', linewidth=2)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vtx88ynrN9C_"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"private_outputs":true,"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}
